{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting JSAnimation\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n",
      "Building wheels for collected packages: JSAnimation\n",
      "  Running setup.py bdist_wheel for JSAnimation ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n",
      "Successfully built JSAnimation\n",
      "Installing collected packages: JSAnimation\n",
      "Successfully installed JSAnimation-0.1\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "observation space: Box(3,)\n",
      "action space: Box(1,)\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "env.seed(2)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, s_size=3, h_size=300, a_size=1):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size)\n",
    "        \n",
    "        self.mu = nn.Linear(h_size, a_size)\n",
    "        self.std = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        \n",
    "        mu = F.tanh(self.mu(x))\n",
    "        std = F.softplus(self.std(x))\n",
    "        return mu, std\n",
    "\n",
    "    def act(self, state, batch_mode=False):\n",
    "        if not batch_mode:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        mu, std = self.forward(state)\n",
    "        \n",
    "        m = Normal(mu, std)\n",
    "        action = m.sample().clamp(-2,2) # obs action space is betwween -2 and 2\n",
    "        \n",
    "        if not batch_mode:\n",
    "            action = action.item() \n",
    "        \n",
    "        return action, m.cdf(action), m.entropy()\n",
    "# run your own policy!\n",
    "policy=Policy().to(device)\n",
    "#policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (fc1): Linear(in_features=3, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (mu): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (std): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "states =  envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07472608, -0.9972041 ,  0.4784946 ],\n",
       "       [ 0.97676342, -0.21432038,  0.46877921],\n",
       "       [ 0.06720443,  0.99773923,  0.09108689],\n",
       "       [-0.99731068, -0.07328989,  0.77271367],\n",
       "       [-0.32019129,  0.94735291,  0.33915817],\n",
       "       [-0.88013181, -0.47472939,  0.44229691],\n",
       "       [-0.94415689,  0.32949623,  0.03627558],\n",
       "       [-0.76469761, -0.6443893 ,  0.49232976]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    states = envs.reset()\n",
    "\n",
    "    states = torch.from_numpy(states).float() \n",
    "\n",
    "    for t in range(tmax):\n",
    "        \n",
    "        actions, probs, entrops = policy.act(states, batch_mode=True)        \n",
    "        states, rewards, dones, _  = envs.step(actions)\n",
    "        states = torch.from_numpy(states).float()\n",
    "        # store the result\n",
    "        state_list.append(states)\n",
    "        reward_list.append(rewards)\n",
    "        prob_list.append(probs.squeeze().cpu().detach().numpy())\n",
    "        action_list.append(actions.squeeze().cpu().detach().numpy())\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if dones.any():\n",
    "            break\n",
    "\n",
    "    #state_list = torch.stack(state_list).squeeze()\n",
    "    #prob_list = torch.stack(prob_list).squeeze()\n",
    "    #action_list = torch.stack(action_list).squeeze()\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, \\\n",
    "        action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_probs, states, actions, rewards = \\\n",
    "             collect_trajectories(envs, policy, tmax=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.from_numpy(np.arange(10).reshape((5,2)))\n",
    "discount = 0.99**np.arange(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7],\n",
       "         [ 8,  9]]),\n",
       " array([ 1.        ,  0.99      ,  0.9801    ,  0.970299  ,  0.96059601]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test, discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # prevents policy to become exactly 0 or 1 helps exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate_continuous(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    \n",
    "    _, new_probs, _ = policy.act(torch.stack(states), batch_mode=True)  \n",
    "    \n",
    "    new_probs = new_probs.squeeze()\n",
    "    # ratio for clipping\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar in /opt/conda/lib/python3.6/site-packages (2.5)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                           | ETA:  3:02:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -1231.045602\n",
      "[ -971.20541383 -1583.80014121 -1069.67028034 -1581.71451825 -1526.56766599\n",
      " -1070.46168981  -982.14652612 -1062.79857795]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                           | ETA:  2:58:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -1253.604319\n",
      "[-1289.46725265 -1380.48150182 -1190.1387016  -1584.51859791 -1011.45376654\n",
      " -1152.25220797 -1454.5734176   -965.94910449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                          | ETA:  2:55:40\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: -1238.770824\n",
      "[ -975.46196931 -1552.01102686  -969.43518177 -1176.37503771 -1002.19351655\n",
      " -1383.68296222 -1254.22733577 -1596.77956141]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                          | ETA:  2:55:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: -1155.663544\n",
      "[-1429.67617695 -1596.63135004  -908.36048009 -1290.11951899 -1087.87324337\n",
      "  -870.51307551 -1066.43260975  -995.70189952]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                         | ETA:  2:53:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: -1343.730509\n",
      "[-1566.89704595 -1448.16034138 -1323.2143045  -1276.67092695  -974.92427005\n",
      " -1457.85002138  -966.85083899 -1735.27632449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                         | ETA:  2:50:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: -1222.853238\n",
      "[-1099.64696668 -1078.99302066  -961.41498511  -894.00269888 -1431.70230637\n",
      " -1035.45372119 -1669.66682132 -1611.94538483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                        | ETA:  2:49:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: -1216.158086\n",
      "[ -898.89993447 -1576.72593061 -1004.98623957 -1322.70277701 -1568.20529525\n",
      "  -977.90880732 -1391.87698869  -987.9587179 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                        | ETA:  2:47:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: -1240.900161\n",
      "[-1390.75980305  -965.67007474 -1586.28481828  -981.69889985 -1068.79255876\n",
      "  -901.93143763 -1694.77478079 -1337.28891176]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                        | ETA:  2:46:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: -1237.011798\n",
      "[-1317.82322976 -1209.47325191 -1207.00236628 -1084.51179589 -1183.28393138\n",
      " -1087.40258315 -1203.68289419 -1602.91433436]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                       | ETA:  2:43:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: -1214.106185\n",
      "[ -891.17693229 -1318.50079417 -1445.96404945 -1314.87429018 -1090.61275462\n",
      "  -967.29009887  -983.13386396 -1701.29669944]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                       | ETA:  2:42:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: -1074.677311\n",
      "[ -998.1851704   -869.76751679  -916.62243176 -1010.02194079 -1260.64651335\n",
      "  -939.37434818 -1623.04202943  -979.75853896]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  2:40:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: -1352.902071\n",
      "[-1110.27058167 -1201.97023907 -1472.99470331  -983.37112622 -1793.23879811\n",
      " -1077.60582792 -1575.49372517 -1608.27156928]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  2:38:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: -1314.130502\n",
      "[-1537.69115672 -1465.99357545 -1058.34811404  -972.38793697 -1371.01088351\n",
      " -1220.79265877 -1532.34913728 -1354.47055107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                     | ETA:  2:36:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: -1206.178833\n",
      "[-1363.62506288 -1024.62009543 -1028.20107385 -1058.8803659   -981.19621563\n",
      " -1692.20564405 -1506.13003009  -994.57217255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                     | ETA:  2:35:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: -1261.655925\n",
      "[ -980.7562829  -1339.21949424 -1076.45688666 -1270.04465957 -1654.41205552\n",
      " -1632.03192569 -1120.03632432 -1020.28977119]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                     | ETA:  2:33:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: -1356.927625\n",
      "[ -982.06826655  -988.03785255 -1663.68907246  -932.51155674 -1697.37919635\n",
      " -1281.91774789 -1764.32113811 -1545.49616718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                    | ETA:  2:31:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: -1272.159060\n",
      "[ -989.13912396 -1163.65064742 -1164.18399162 -1543.00875518 -1605.40221375\n",
      " -1186.80935774  -992.26994833 -1532.80844414]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                    | ETA:  2:29:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: -1145.519821\n",
      "[ -988.56883401 -1694.03247227 -1168.93060334 -1120.988704    -995.60354662\n",
      " -1430.00621911  -891.08387037  -874.94431538]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  19% |########                                   | ETA:  2:27:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: -1069.067353\n",
      "[-1067.81811742  -998.37176541  -908.27232024 -1581.88583254  -872.59805633\n",
      "  -881.9657194  -1280.39825867  -961.22875532]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  2:25:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: -1149.124252\n",
      "[-1188.99172649  -973.59159837 -1588.92032409 -1044.63928656 -1144.92893454\n",
      " -1076.64660139 -1377.20898714  -798.06655914]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |#########                                  | ETA:  2:24:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: -1187.553023\n",
      "[-1586.58855696 -1075.667503    -859.30239651 -1552.90391107  -976.87434857\n",
      " -1322.97315955  -869.11262895 -1257.001677  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |#########                                  | ETA:  2:22:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: -1212.730123\n",
      "[-1014.06798314  -865.74333246 -1298.50192153 -1702.25274586 -1280.43849212\n",
      "  -849.29804318 -1187.35969908 -1504.17876695]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |#########                                  | ETA:  2:20:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: -1106.744213\n",
      "[-1476.61336968 -1043.25355075 -1086.04994936  -980.78318274  -994.203725\n",
      " -1176.01549956  -979.42736769 -1117.60705612]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                 | ETA:  2:18:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: -1269.213181\n",
      "[-1335.70017025  -965.84060689 -1402.22642437 -1477.34354171 -1628.06492078\n",
      " -1109.01797882 -1385.32001067  -850.19179705]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  25% |##########                                 | ETA:  2:16:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: -1245.444074\n",
      "[-1479.01943787 -1002.76672863 -1564.499391   -1425.76222486 -1631.46647512\n",
      "  -894.1732522   -983.56383932  -982.30123981]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  2:14:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 520, score: -1305.836121\n",
      "[-1067.00366122 -1081.58331513 -1151.85310246 -1479.96541074 -1590.33643761\n",
      " -1420.86878898 -1002.44065765 -1652.63759151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                                | ETA:  2:12:56\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 540, score: -1082.742095\n",
      "[ -955.56736851  -967.71390943  -834.94822663 -1086.14652868  -873.25379497\n",
      " -1211.8569858  -1193.6019797  -1538.84796791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |############                               | ETA:  2:11:10\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 560, score: -1186.271833\n",
      "[-1593.86834202 -1084.28051473 -1051.25131171  -788.1981303  -1541.55336486\n",
      "  -961.72970706 -1201.95378292 -1267.33950979]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                               | ETA:  2:09:26\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 580, score: -1284.146110\n",
      "[-1024.55178169 -1026.77097806 -1516.72291729 -1205.0121975  -1480.43615104\n",
      " -1629.01355816 -1315.81617787 -1074.84511668]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                               | ETA:  2:07:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600, score: -1136.731451\n",
      "[-1180.82985039 -1002.25888876  -885.19790868  -966.07803037 -1197.60168905\n",
      "  -899.57765432 -1368.47397081 -1593.83361503]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  31% |#############                              | ETA:  2:05:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 620, score: -1152.758317\n",
      "[-1081.59479908 -1485.3402503  -1197.31110261 -1176.28566338  -875.27270034\n",
      " -1095.73496579 -1312.66041923  -997.86663791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  2:03:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 640, score: -1285.443108\n",
      "[-1349.52682538 -1388.22189097 -1500.3597072  -1360.83382211 -1219.1449741\n",
      "  -931.09510624 -1051.62123469 -1482.74130698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                             | ETA:  2:02:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 660, score: -1316.742350\n",
      "[-1537.66093553 -1610.68639936 -1088.44862568 -1296.64018297 -1578.9440308\n",
      " -1609.4468893   -952.543213    -859.56852297]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  34% |##############                             | ETA:  2:00:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 680, score: -1327.250777\n",
      "[-1054.97900696  -971.71153439 -1710.32022819 -1442.63633267 -1015.7318176\n",
      " -1461.32908427 -1533.74969202 -1427.54851879]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  35% |###############                            | ETA:  1:58:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700, score: -1209.919486\n",
      "[-1562.10572067 -1374.75560369 -1070.96280406 -1230.03322017  -998.6241741\n",
      " -1085.40686246 -1485.27532305  -872.19218219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  1:56:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 720, score: -1323.041588\n",
      "[-1599.68745628 -1592.25375223 -1518.41612485  -761.64989352 -1007.92681588\n",
      " -1746.70241686 -1493.27984658  -864.41639392]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  1:54:54\r"
     ]
    }
   ],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "# training loop max iterations\n",
    "episode = 2000\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('Pendulum-v0', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        #L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "        #                                  epsilon=epsilon, beta=beta)\n",
    "    \n",
    "        L = -clipped_surrogate_continuous(policy, old_probs, states, actions, rewards,\n",
    "                                          epsilon=epsilon, beta=beta)\n",
    "        #print(L.grad_fn)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
